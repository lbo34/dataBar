---
title: Reading Notes Series - Applied Predictive Modeling - Section 8 (Regression
  Trees and Rule-Based Models - Part I)
author: Laurent Barcelo
date: '2018-12-12'
slug: reading-notes-series-applied-predictive-modeling-section-8-regression-trees-and-rule-based-models
categories:
  - Reading Notes
tags:
  - trees
  - CART
  - Conditinal Inference Trees
description: ''
featured_image: ''
---

This first series is related to M. Kuhn and K. Johnson book "Applied Predictive Modeling". The book pdf is available [here](https://vuquangnguyen2016.files.wordpress.com/2018/03/applied-predictive-modeling-max-kuhn-kjell-johnson_1518.pdf), but if, like me, you prefer to have a paper copy of the book, you can purchase it [here](https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=sr_1_1?ie=UTF8&qid=1540208647&sr=8-1&keywords=applied+predictive+modeling). **This post is dedicated to section 8 on regression trees and rules based models** and as a **part I, it is dealing with basic trees**.

<img src="/post/2018-10-22-reading-notes-series-applied-predictive-modeling-section-3_files/Applied Predictive Modeling Cover.png" alt="Book cover" width="400px"/>

This chapter is a very important chapter of the book as it goes through **basic trees, ensemble methods such as random forests, boosting and cubist**. As someone who had little blackground in these techniques (beyond what is given in the DataCamp courses in the Machine Learning with R track), it is a quite difficult chapter to swallow (which is the reason why it is split in several parts in this blog).
</br>
</br>
**Introduction**

As its roots, a tree is a series of nested if-then statements. For example:

`if Predictor A >= 1.7 then`  
`| if Predictor B >= 202.1 then Outcome = 1.3`  
`| else Outcome = 5.6`   
`else Outcome = 2.5`  `

In this examples, there are **2 splits** (one on predictor A and one on predictor B) and **3 terminal nodes** (corresponding to the 3 outcomes). The terminal nodes can be either **categorical** (classification tree) or **numeric** (regression tree). They can also be a more complex equation than a single value. Each terminal node correspond to a specific **rule** which is the corresponding un-nested version of the if-then statement :

rule 1: if Predictor A >= 1.7 and Predictor B >= 202.1 then Outcome = 1.3  
rule 2: if Predictor A >= 1.7 and Predictor B < 202.1 then Outcome = 5.6  
rule 3: if Predictor A < 1.7 then Outcome = 2.5

In the case where the predictors are numeric, the same predictor can be involved in several splits involving different threshold values.

Trees are **very popular**. They are **easy to interpret** and they are **easy to implement**. They can handle different types of predictors (including sparse, skwewed, categorical...) **without pre-treatment**. There is no pre-conception of the form of the relation between Y and the Xi.

However, trees are also **prone to large instabilities** (small changes in conditions can lead to radical changes in the trees). Also, Their **prediction performance can be low**. These two drawbacks led to the development of **ensemble methods that consider multiple trees** rather than a single one to predict a specific outcome.
</br>
</br>
**Basic Regression Trees**

In regression trees, we seek to **split the predictors space into homogeneous pieces** with regard to the response. The parameters of the trees are:

* The **predictors to split on** the the **values** on which to split;
* The tree **depth**;
* The **prediction equation** in the terminal nodes.

For basic regression trees, **the response is continuous** and the prediction equation in the terminal nodes is **a simple constant**. 

One of the most popular version is these trees is called **Classification and Regression Trees** (**CART**). In the CART methodology, there is a **growing step** which is followed by a **pruning step**.

In the growing step, the trees branches are determined according to **recursive partitionning**: for a dataset S, we **scan every predictor and every possible split value** in the predictors that split the datasets in two ($S_1$ and $S_2$) in such a way that the sum of the square of the errors (See equation below) is minimized. 

$$\text{SSE} =  \sum_{i \in S_1}(y_i-\bar{y_1})^2 + \sum_{i \in S_2}(y_i-\bar{y_2})^2$$
This process continues for splitting $S_1$ and $S_2$ into smaller spaces. The growing step continues until we have reached a termination criteria such as maximum tree depth or minimum amount of samples in the terminal nodes.

The authors provide an example, again with the now familiar case study dataset. For the solubility dataset, an example of split on the `Number of Carbon Atoms` variable is shown in the figure below. We can determine that the split that reduces the most the SSE is at 3.78... As this is the combination of predictor and split values that most reduces the SSE, we can derive the first split of the tree in this way:

`if Number of Carbon Atoms >= 3.78 then Solubility = -4.49`     
`else Solubility = -1.84`

![](/Reading-Notes/2018-12-12-reading-notes-series-applied-predictive-modeling-section-8-regression-trees-and-rule-based-models_files/First split on solubility dataset.png)

The figure below shows the next steps in the tree growing. One branch of the tree involves a split on the `SurfaceArea2` variable, while the other one involves a split on the `MolWeight` variable.
![](/Reading-Notes/2018-12-12-reading-notes-series-applied-predictive-modeling-section-8-regression-trees-and-rule-based-models_files/CART Tree growing step.png)

At the end of the growing step, the tree is likely to be **very large** and to **overfit** the response. This is what we want to address with the pruning step. This is done through a **cost-complexity tuning** that involves a version of the SSE **penalized for the number of terminal nodes**:

$$ \text{SSE}_{c_p} = \text{SSE} + c_p.(\text{# terminal nodes})$$
where $c_p$ is a complexity parameter. 

For smaller values of $c_p$, the trees are larger. In order to find the optimal value of $c_p$, the parameter can tuned in a cross-validation procedure using a one-standard-error rule (see [section 4](https://www.lbo34.com/reading-notes/reading-notes-series-applied-predictive-modeling-section-4-over-fitting-and-model-tuning/)). Using this methodology, the authors determined that the optimal value of $c_p$ is 0.01 and the final tree has 11 terminal nodes and a RMSE of 1.05. An alternative would be to use the smallest possible cross-validated RMSE. This would lead in this example to a much larger tree ($c_p = 0.003$, 25 terminal nodes) and a RMSE of 0.97.

![](/Reading-Notes/2018-12-12-reading-notes-series-applied-predictive-modeling-section-8-regression-trees-and-rule-based-models_files/Final CART tree.png)

Once the tree is grown and pruned, we can determine the **relative importance of the predictors**. One approach is to keep track of what is the **amount of reduction of SSE associated with each split**. Intuitively, predictor splits that happens earlier in the growing process or predictor splits that occur multiple times in the tree correspond to larger importance. For the final tree, the authors have determined that the relative predictors importance is the following:

![](/Reading-Notes/2018-12-12-reading-notes-series-applied-predictive-modeling-section-8-regression-trees-and-rule-based-models_files/CART predictors importance.png)


The advantage of trees, when their size remains reasonnable, is that there are **simple and highly interpretable**. Their **computation is quick**. They also lead to **feature selection** which is a great advantage when colinearity among predictors is not too high (otherwise the choice of a predictor over another one is somewhat random)... Colinearity can also dilute the parameter importance as it may involve 2 splits with 2 correlated predictors while 2 splits with a single predictors could be equivalent.

On the other side, trees have disadvantages:

* because there are so simple, trees can have **sub-optimal prediction performance**. In this example, the final tree has 11 terminal nodes, which means that it only leads to 11 discrete values for the prediction of the response. 
* as mentionned above, such trees can be highly **unstable**. 
* CART trees are affected by a **selection bias**: predictors with more discrete values have a larger likelihood to be selected. In particular, this can be a problem when *noise variables* have more distinct values than *informative variables*. The variable importance graph abobe shows that continuous predictors tend to have a larger importance than binary predictors (FPxxx).

There are **several methodologies leading to unbiased trees**. One possible approach called **GUIDE** decouples the choice of the predictor to split on (using a statistical hypoothesis) and the split value. Another approach is called **Conditional inference trees**. This general approach can be used for regression and also for classification. For each candidate split, a statistical test is performed to evaluate the difference between the 2 subgroups and a **p-value** is associated. This allows to compare splits for predictors that are on different scales. In this procedure, the predictors are increasingly penalized by the multiple comparisons when there are a large number of possible split values, reducing the selection bias. These conditional inference trees **do not require a pruning step**: when the number of samples in the nodes decrease, that statistical power decreases and the p-value tend to increase which prevents the tree from becoming too complex and overfit.

Rather than fixing arbitrarily a p-value (i.e 0.05 for instance) and evaluating the corresponding conditional inference tree, the authors suggest that **a better methodology is to tune the p-value** according to the model performance. The following figure shows that tuned p-value woudl correspond to $1-p = 0.853$ and has a corresponding RMSE of 0.92. The corresponding tree has 32 terminal nodes, which is larger than the CART trees described above.

![](/Reading-Notes/2018-12-12-reading-notes-series-applied-predictive-modeling-section-8-regression-trees-and-rule-based-models_files/p-value tuning for conditional inference trees.png)
</br>
</br>
**R code**

The following code allows to **build a CART tree of depth 2** (using the `rpart` package) and to **display it nicely** (using the `partykit` package)

```{r}
library(AppliedPredictiveModeling)
data(solubility)

# Constructing a tree of depth 2 and displaying it:
library(rpart)
trainData <- solTrainXtrans
trainData$y <- solTrainY
rpSmall <- rpart(y ~ ., data = trainData, 
                 control = rpart.control(maxdepth = 2))
rpSmall

### Use the partykit package to make some nice plots. First, convert
### the rpart objects to party objects.
suppressMessages(library(partykit))
rpSmallTree <- as.party(rpSmall)
plot(rpSmallTree)
```

The following code allows build an **optimal CART tree by tuning the value of $c_p$** using the `caret` package:

```{r}
suppressMessages(library(caret))
set.seed(100)
indx <- createFolds(solTrainY, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = indx)

set.seed(100)
cartTune <- train(x = solTrainXtrans, y = solTrainY,
                  method = "rpart",
                  tuneLength = 25,
                  trControl = ctrl)
cartTune

#displaying (but it is NOT very nice...)
cartTree <- as.party(cartTune$finalModel)
plot(cartTree)

#Variable importance
cartImp <- varImp(cartTune, scale = FALSE, competes = FALSE)
cartImp

# Saving predictions
testResults <- data.frame(obs = solTestY,
                          CART = predict(cartTune, solTestXtrans))
```

The folliwing code allows to **tune a conditional inference tree along a range of $1-p$ values**:

```{r}
### Tune the conditional inference tree
cGrid <- data.frame(mincriterion = sort(c(.95, seq(.75, .99, length = 15))))

set.seed(100)
ctreeTune <- train(x = solTrainXtrans, y = solTrainY,
                   method = "ctree",
                   tuneGrid = cGrid,
                   trControl = ctrl)
ctreeTune
plot(ctreeTune)

##ctreeTune$finalModel               
plot(ctreeTune$finalModel) #as.party not possible here.


## saving prediction
testResults$cTree <- predict(ctreeTune, solTestXtrans)
```

